{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw, re, nltk, pickle, string, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from praw.models import MoreComments\n",
    "from cfg import config\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "stop_words = list(set(stopwords.words('english'))) + [\"the\"]\n",
    "cfg = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = cfg.client_id,\\\n",
    "                     username = cfg.username, \\\n",
    "                     password = cfg.password,\\\n",
    "                     client_secret = cfg.secret,\\\n",
    "                     user_agent = cfg.agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_comments(usernames, get_other_users = True, verbose = True):\n",
    "    comments = {}\n",
    "    threads = []\n",
    "    not_bot_users = []\n",
    "    \n",
    "    stime = time.time()\n",
    "    n_unames = len(usernames)\n",
    "    pct = 0\n",
    "    if verbose: print(\"Started\", time.strftime(\"%a, %d %b %Y %H:%M:%S %Z\", time.localtime()))\n",
    "    for i, u in enumerate(usernames):\n",
    "        try:\n",
    "            user = reddit.redditor(u)\n",
    "            comments[u] = []\n",
    "            for c in user.comments.new(limit=None):\n",
    "                cc = clean_comment(c.body)\n",
    "                if len(cc) > 0:\n",
    "                    comments[u] += [cc]\n",
    "                if get_other_users:\n",
    "                    thread = c.submission\n",
    "                    if not thread in threads:\n",
    "                        threads.append(thread)\n",
    "                        not_bot_users += [list(set([x.author.name for x in thread.comments.list() \\\n",
    "                                                   if not type(x) == MoreComments and x.author and \\\n",
    "                                                   not x.author.name in cfg.bot_names]))]\n",
    "                \n",
    "            if 100*i/len(usernames) > pct:\n",
    "                pct += 2.5\n",
    "                print(\"Finished %0.1f percent of users in %0.1f minutes.\" % (100*i/len(usernames), (time.time()-stime)/60))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if verbose: \n",
    "                print(\"Barfed on\", u)\n",
    "                print(e)\n",
    "\n",
    "    if verbose: print('Finished in %0.1f minutes' % ((time.time()-stime)/60))\n",
    "    #return transform_and_lemmatize(comments), threads\n",
    "    return comments, threads, not_bot_users\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_users(ulist):\n",
    "    all_users = [name for thread_authors in ulist for name in thread_authors]\n",
    "    unique_users = list(set(all_users))\n",
    "    uname_incidences = [all_users.count(x) for x in unique_users]\n",
    "    sorted_unames = [name for _,name in sorted(zip(uname_incidences, unique_users), reverse=True)]\n",
    "    print(len(unique_users), len(uname_incidences))\n",
    "    return sorted_unames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment_string):\n",
    "    lines = comment_string.split(\"\\n\")\n",
    "    clean_lines = []\n",
    "    for line in lines:\n",
    "        words = []\n",
    "        if line.startswith(\">\"):\n",
    "            pass\n",
    "        else:\n",
    "            for w in line.split():\n",
    "                if w.startswith((\"[\", \"http\", \"www\", \"@\", \"#\")):\n",
    "                    pass\n",
    "                else:\n",
    "                    words.append(w.replace(\"\\n\\n\",\" \").replace(\"Â°\",\"\"))\n",
    "            clean_lines.append(\" \".join(words))\n",
    "        \n",
    "    return \"\\n\".join(clean_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_vectors(comments, vectorizer = None, remove_stops = True, remove_punc = True, fit = True):\n",
    "    if vectorizer == None:\n",
    "        vectorizer = CountVectorizer()\n",
    "    lemmas = transform_and_lemmatize(comments, remove_stops, remove_punc)\n",
    "    if fit:\n",
    "        vector = vectorizer.fit_transform(lemmas)\n",
    "    else:\n",
    "        vector = vectorizer.transform(lemmas)\n",
    "    return vector, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_and_lemmatize(comments, remove_stops = True, remove_punc = True):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    output = []\n",
    "    for c in comments:\n",
    "        if remove_punc:\n",
    "            c = remove_punctuation(c)\n",
    "        tokens = word_tokenize(c)\n",
    "        if remove_stops:\n",
    "            filtered_tokens = remove_stop_words(tokens)\n",
    "        else:\n",
    "            filtered_tokens = tokens\n",
    "        lemmas = lemmatize(filtered_tokens, stemmer)\n",
    "        output.append(\" \".join(lemmas))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(comment):\n",
    "    for p in string.punctuation:\n",
    "        comment = comment.replace(p,\" \")\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(word_list):\n",
    "    return [word for word in word_list if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(comment, stemmer):\n",
    "    parts_of_speech = [get_wordnet_pos(word) for word in comment]\n",
    "    output = [stemmer.lemmatize(word, pos) for word, pos in zip(comment, parts_of_speech)]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test, pred):\n",
    "    labels = np.asarray(['Not Bot', 'Bot'])\n",
    "    cm = confusion_matrix(labels[test], labels[pred], labels)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm)\n",
    "    plt.title('Confusion matrix of the classifier')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + list(labels))\n",
    "    ax.set_yticklabels([''] + list(labels))\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_commenting_users(comment_dict, min_comments = 0, min_words = 0):\n",
    "    if not min_words:\n",
    "        return {x[0]:x[1] for x in comment_dict.items() if len(x[1]) >= min_comments}\n",
    "    else:\n",
    "        outdict = {}\n",
    "        for uname, comments in comment_dict.items():\n",
    "            if sum([len(x.split()) for x in comments]) > min_words:\n",
    "                outdict[uname] = comments\n",
    "        return outdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got comments from 128 bots and 99 other users.\n"
     ]
    }
   ],
   "source": [
    "with open(\"not_bot_comments_0-150.pkl\", \"rb\") as f:\n",
    "    not_bot_dict = pickle.load(f)\n",
    "with open(\"bot_comments_dict.pkl\", \"rb\") as f:\n",
    "    bot_reddit_dict = pickle.load(f)\n",
    "\n",
    "bot_reddit_dict = remove_non_commenting_users(bot_reddit_dict, min_words=40)\n",
    "not_bot_dict = remove_non_commenting_users(not_bot_dict, min_words=1000)\n",
    "\n",
    "bot_reddit_corpi = [\" \".join(bot_reddit_dict[x]) for x in bot_reddit_dict.keys()]\n",
    "not_bot_reddit_corpi = [\" \".join(not_bot_dict[x]) for x in not_bot_dict.keys()]\n",
    "print(\"Got comments from %d bots and %d other users.\" % (len(not_bot_dict.keys()), len(bot_reddit_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Fri, 30 Aug 2019 14:47:58 EDT\n",
      "Getting vectors took 1.2 minutes.\n"
     ]
    }
   ],
   "source": [
    "stime = time.time()\n",
    "print(\"Started\", time.strftime(\"%a, %d %b %Y %H:%M:%S %Z\", time.localtime()))\n",
    "features, vectorizer = get_text_vectors(bot_reddit_corpi + not_bot_reddit_corpi, vectorizer=TfidfVectorizer(), remove_stops = True)\n",
    "labels = np.asarray([1]*len(bot_reddit_corpi) + [0]*len(not_bot_reddit_corpi))\n",
    "print(\"Getting vectors took %0.1f minutes.\" % ((time.time()-stime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Fri, 30 Aug 2019 14:50:30 EDT\n",
      "Performance metrics for fold 0:\n",
      "Accuracy:  81.6\n",
      "Precision: 100.0\n",
      "Recall:    58.8\n",
      "\n",
      "Performance metrics for fold 1:\n",
      "Accuracy:  92.1\n",
      "Precision: 100.0\n",
      "Recall:    78.6\n",
      "\n",
      "Performance metrics for fold 2:\n",
      "Accuracy:  97.4\n",
      "Precision: 94.4\n",
      "Recall:    100.0\n",
      "\n",
      "Performance metrics for fold 3:\n",
      "Accuracy:  94.7\n",
      "Precision: 100.0\n",
      "Recall:    88.9\n",
      "\n",
      "Performance metrics for fold 4:\n",
      "Accuracy:  92.1\n",
      "Precision: 91.7\n",
      "Recall:    84.6\n",
      "\n",
      "Performance metrics for fold 5:\n",
      "Accuracy:  86.5\n",
      "Precision: 89.5\n",
      "Recall:    85.0\n",
      "\n",
      "Average accuracy:  90.7\n",
      "Average precision: 95.9\n",
      "Average recall:    82.6\n",
      "Time to completion: 6.2 min\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=6, shuffle=True, random_state=90)\n",
    "correct_probs = []\n",
    "incorrect_probs = []\n",
    "accs = []\n",
    "preciss = []\n",
    "recalls = []\n",
    "i=0\n",
    "\n",
    "stime = time.time()\n",
    "print(\"Started\", time.strftime(\"%a, %d %b %Y %H:%M:%S %Z\", time.localtime()))\n",
    "\n",
    "for train_indices, test_indices in kf.split(features):\n",
    "    features_train = features[train_indices]\n",
    "    features_test = features[test_indices]\n",
    "    labels_train = labels[train_indices]\n",
    "    labels_test = labels[test_indices]\n",
    "    clf = GradientBoostingClassifier(learning_rate=0.33, n_estimators=500)\n",
    "    clf.fit(features_train.toarray(), labels_train)\n",
    "    pred = clf.predict(features_test.toarray())\n",
    "    pred_probs = clf.predict_proba(features_test.toarray())\n",
    "    error_inds = (labels_test==pred)\n",
    "    correct_probs += [x[1] for x in pred_probs[~error_inds]]\n",
    "    incorrect_probs += [x[1] for x in pred_probs[error_inds]]\n",
    "    acc = 100*accuracy_score(labels_test, pred)\n",
    "    prec = 100*precision_score(labels_test, pred)\n",
    "    recall = 100*recall_score(labels_test, pred)\n",
    "    accs.append(acc)\n",
    "    preciss.append(prec)\n",
    "    recalls.append(recall)\n",
    "    print(\"Number of bots in training set:         \", sum(labels_train))\n",
    "    print(\"Number of regular users in training set:\", len(labels_train)-sum(labels_train))\n",
    "    print(\"Ratio of bots to regular users:          %0.1f\" % (sum(labels_train)/(len(labels_train)-sum(labels_train))))\n",
    "    print(\"Performance metrics for fold %d:\" % (i))\n",
    "    print(\"Accuracy:  %0.1f\" % (acc))\n",
    "    print(\"Precision: %0.1f\" % (prec))\n",
    "    print(\"Recall:    %0.1f\" % (recall))\n",
    "    print()\n",
    "    i += 1\n",
    "\n",
    "print(\"Average accuracy:  %0.1f\" % (sum(accs)/len(accs)))\n",
    "print(\"Average precision: %0.1f\" % (sum(preciss)/len(preciss)))\n",
    "print(\"Average recall:    %0.1f\" % (sum(recalls)/len(recalls)))\n",
    "print(\"Time to completion: %0.1f min\" % ((time.time()-stime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.33, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "              n_iter_no_change=None, presort='auto', random_state=None,\n",
       "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started geting vectors at Fri, 30 Aug 2019 14:59:41 EDT\n",
      "Getting vectors took 3.1 minutes.\n"
     ]
    }
   ],
   "source": [
    "with open(\"not_bot_comments_all.pkl\", \"rb\") as f:\n",
    "    all_not_bot_dict = pickle.load(f)\n",
    "\n",
    "all_not_bot_dict = remove_non_commenting_users(all_not_bot_dict, min_words = 1000)\n",
    "all_not_bot_reddit_corpi = [\" \".join(all_not_bot_dict[x]) for x in all_not_bot_dict.keys()]\n",
    "len(all_not_bot_reddit_corpi)\n",
    "stime = time.time()\n",
    "print(\"Started geting vectors at\", time.strftime(\"%a, %d %b %Y %H:%M:%S %Z\", time.localtime()))\n",
    "normal_features, normal_vectorizer = get_text_vectors(all_not_bot_reddit_corpi, vectorizer=vectorizer, fit=False, remove_stops = True)\n",
    "normal_labels = np.asarray([0]*len(all_not_bot_reddit_corpi))\n",
    "print(\"Getting vectors took %0.1f minutes.\" % ((time.time()-stime)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly classified 2.2 percent of innocent users as bots.\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(normal_features)\n",
    "pred_probs = clf.predict_proba(normal_features)\n",
    "print(\"Incorrectly classified %0.1f percent of innocent users as bots.\" % (100*sum(pred)/len(pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_classifier_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump([clf, vectorizer], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
